% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Monte Carlo Health Attuned Multiple Metrics Evaluation Rubric - preliminary tests},
  pdfauthor={Nonie},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Monte Carlo Health Attuned Multiple Metrics Evaluation Rubric -
preliminary tests}
\author{Nonie}
\date{09/03/2020}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Clusters can be evaluated by comparing their distribution to that of a
null distribution, however several parameters in that need to be tested
first, for example what statistic to compare the original data set and
the null distribution generation. Below we start the initial testing of
those parameters on simple known cluster data sets.

\hypertarget{method}{%
\subsection{Method}\label{method}}

\hypertarget{null-distribution}{%
\subsubsection{Null Distribution}\label{null-distribution}}

Three methods were chosen to create null distributions from the data

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Shuffle the data} takes all the original data points and
  shuffles the order in the variables to remove correlation between the
  variables
\item
  \textbf{Max Min Uniform Distribution} is generated from a uniform
  distribution between the minumum and maximum values of the variable
\item
  \textbf{PCA Distribution} takes the eigan vectures of the data set are
  gained through PCA, these are then used to transform a random data set
  generated from a single gaussian distribution. The resulting data set
  is one with only one cluster yet maintains the relationships between
  the variables
\end{enumerate}

To create a null distribution, 500 test data sets were generated

\hypertarget{cluster-seperation-metrics}{%
\subsubsection{Cluster Seperation
Metrics}\label{cluster-seperation-metrics}}

Three Seperation metrics are used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Huberts Gamma Statistic} is a measure of how much the high
  distances between variables correlates with cluster membership. It
  uses 2 matrices the distance matrix which was the basis of clustering
  (D) and a matrix recording cluster membership where the value at point
  (i,j) is 1 if they are from different clusters and 0 if they are from
  the same. The statistic = the sum of D(i,j) * C(i,j) for i in 1-n and
  j in 2 -n / the number of point pairs. The higher the value the better
  cluster structure
\item
  \textbf{Normalised Gamma Statistic} is the normalised version of the
  statistic above. The statistic = (the sum of D(i,j)-mean(D) * C(i,j)-
  mean(C) for i in 1-n and j in 2 -n / the number of point
  pairs)/var(D)*var(C). This returns a value between 0 and 1 with high
  being more clustered
\item
  \textbf{Total Within Cluster Sum of Squares} is the sum of the
  distances from each point to its assigned cluster center, the smaller
  the distance the better.
\end{enumerate}

\hypertarget{cluster-methodology}{%
\subsubsection{Cluster Methodology}\label{cluster-methodology}}

We apply k-means to each data set using a k++ initialisation with 50
resamples which then returns the optimum result

\hypertarget{test-data-generation}{%
\subsubsection{Test Data Generation}\label{test-data-generation}}

The data was generated using SciKit learn Make Classifications function
from the datasets module. 4 parameters of the data are altered we used a
full factorial experimental design:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Number of clusters - 2,4,5
\item
  Number of features - 10,20
\item
  \% Noise features - 0\% 10\% 50\%
\item
  Seperation (measured in size of hypercube between clusters) - 0.5,1,3
\end{enumerate}

This resulted in 54 distinct data sets.

\hypertarget{overall-experiment-structure}{%
\subsubsection{Overall Experiment
Structure}\label{overall-experiment-structure}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  54 Datasets were created
\item
  For each data set 500 null distributions were made with each null
  distribution method (total 1500 null distributions
\item
  K-means was run on the original data set and 1500 null datasets and
  the three cluster seperation metrics were returned, for k = 2-6
\item
  The mean and standard deviation is returned for each null distribution
  method, for each seperation metric and for each cluster number
\item
  The seperation metric score and p value for the original data set is
  returned for each null distribution method, for each seperation metric
  and for each cluster number
\end{enumerate}

\hypertarget{experiment-outcomes}{%
\subsubsection{Experiment Outcomes}\label{experiment-outcomes}}

Each distribution method and Cluster metric will by the accuracy of
identifying the correct cluster number

\#\#Results

\hypertarget{results}{%
\section{Results}\label{results}}

Figure 1 shows the senstivity for each metric, null distribution
combination. The first thing to note is that using the within sum of
squares was unsuccsesfull no matter what the distribution method used.
The best method used was the combination of random order generation and
huberts gamma statistic with a sensitivity of .5 which is still pretty
bad. Overall out of the data genration methods random order performed
the best, follewed by pca then lastly min max.

\includegraphics{sand_box_write_up_files/figure-latex/figure 1-1.pdf}

Figure 2 shows how many times each method distribution pairing
identified the correct cluster number and did not identify any other
cluster number as significant, broken down by seperation and ratio of
noise varabibles (max 3). As the ratio of noise variables increasesand
the seperation value decreases (top right of each figure) the clustering
problem gets harder.

\includegraphics{sand_box_write_up_files/figure-latex/figure 2-1.pdf}

Figure 2 shows huberts gamma statistic performs better than the other 2
metrics and shows a split between random order being better at
identifying the harder cluster problems with smaller seperation, and pca
better at solving the easier ones. This could be because if there a
large seperation in the data already there will also be in the null
distribution as it only uses the values that exist.

Figure 3 shows how many times the method, distributer pairing thought
there were clusters there (for k = 2 -6). What it shows is within
cluster sum of squares unable to desern between clustered and null
distributions whatsoever, however the issue with hubers random order and
norm min max seem that it is finds clusters when they are not there.

\includegraphics{sand_box_write_up_files/figure-latex/figure 3-1.pdf}
One potential reason for the methods not finding hte correct cluster
number is that k-means did a terrible job of identifying the clusters,
so we compared the mean matching score between the k means cluster
labels and the original cluster. This is shown in figure 4. It appears
from this plot that k-means is partly responsible for not being able to
identify the correct cluster number

\includegraphics{sand_box_write_up_files/figure-latex/figure 4-1.pdf}

\hypertarget{going-forward}{%
\section{Going Forward}\label{going-forward}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use PCA before K-means with greater number of random starts to improve
  performance
\item
  Test more cluster metrics (drop tss)
\item
  Return metrics on the distributions namely kertosis
\end{enumerate}

\end{document}
